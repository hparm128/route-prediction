{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/haleyparmley/nfl-big-data-bowl-2025-predicting-receiver-routes?scriptVersionId=215855897\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"45f0bf10","metadata":{"papermill":{"duration":0.006618,"end_time":"2025-01-02T22:11:54.038853","exception":false,"start_time":"2025-01-02T22:11:54.032235","status":"completed"},"tags":[]},"source":["# NFL Big Data Bowl 2025: Predicting Receiver Routes from Pre-Snap Data\n","\n","## **Objective**\n","The goal of this project is to predict the route a receiver will run using pre-snap data. This analysis focuses on leveraging player tracking data, game statistics, and team tendencies to develop robust predictive models.\n","\n","---\n","\n","## **Data and Approach**\n","We will:\n","1. Merge player, play, and tracking data to create a comprehensive dataset.\n","2. Preprocess the data for training a machine learning models.\n","3. Use classification techniques to predict the `routeRan` variable, representing the receiver's route.\n","\n","---\n"]},{"cell_type":"markdown","id":"d7ed498a","metadata":{"papermill":{"duration":0.005047,"end_time":"2025-01-02T22:11:54.050508","exception":false,"start_time":"2025-01-02T22:11:54.045461","status":"completed"},"tags":[]},"source":["## **Introducing the Random Forest Model for the NFL Big Data Bowl 2025**\n","\n","In this competition, the goal is to predict the route a receiver will run based on pre-snap data. To tackle this problem, we use a **Random Forest Classifier**, a robust and interpretable machine learning model.\n","\n","#### Why Random Forest?\n","- **Interpretability**: Random Forest provides insights into feature importance, helping us understand which pre-snap features influence receiver routes.\n","- **Versatility**: It handles a mix of numerical and categorical data effectively, making it suitable for this dataset.\n","- **Imbalance Handling**: The `class_weight` parameter helps address class imbalance, ensuring fair predictions for all route types.\n","\n","#### Key Steps:\n","1. Preprocessing:\n","   - Encoding categorical features.\n","   - Scaling numerical features for consistency.\n","2. Training:\n","   - The Random Forest model is trained on historical data, learning patterns from pre-snap features.\n","3. Evaluation:\n","   - The model's accuracy, F1 score, and classification report are analyzed to assess performance.\n","4. Visualization:\n","   - Feature importance is visualized to identify the most influential pre-snap features.\n","\n","This model serves as a baseline for predicting receiver routes, offering interpretability and reliable performance while paving the way for further optimization and enhancements.\n"]},{"cell_type":"markdown","id":"acfb28e1","metadata":{"papermill":{"duration":0.005013,"end_time":"2025-01-02T22:11:54.0611","exception":false,"start_time":"2025-01-02T22:11:54.056087","status":"completed"},"tags":[]},"source":["## **Step 1: Importing Libraries and Initial Setup**\n","\n","This step sets up the essential libraries and configurations required for the data preprocessing and splitting process:\n","\n","1. **Importing Libraries**:\n","   - **Pandas**: Used for data manipulation and analysis, including reading and merging datasets.\n","   - **Scikit-learn**:\n","     - `train_test_split` for splitting the dataset into training and testing sets for machine learning.\n","   - **OS**: Facilitates file path management and access for loading and saving datasets.\n","\n","2. **Configure Logging**:\n","   - Logging is configured to provide a clear and structured output of execution progress.\n","   - Logs include timestamps, logging levels (e.g., INFO, ERROR), and messages for both successful operations and debugging.\n","\n","This foundational setup ensures the environment is ready for handling data and splitting it into subsets for further processing.\n"]},{"cell_type":"code","execution_count":1,"id":"eff39387","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:11:54.073542Z","iopub.status.busy":"2025-01-02T22:11:54.073185Z","iopub.status.idle":"2025-01-02T22:11:55.911789Z","shell.execute_reply":"2025-01-02T22:11:55.910656Z"},"papermill":{"duration":1.847343,"end_time":"2025-01-02T22:11:55.913793","exception":false,"start_time":"2025-01-02T22:11:54.06645","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import logging\n","import os\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"]},{"cell_type":"markdown","id":"5c18c0fb","metadata":{"papermill":{"duration":0.005166,"end_time":"2025-01-02T22:11:55.925179","exception":false,"start_time":"2025-01-02T22:11:55.920013","status":"completed"},"tags":[]},"source":["## **Step 2: Loading and Preparing Data**\n","\n","This function processes and consolidates data from multiple CSV files, performs feature engineering, and prepares the data for modeling. The key steps are:\n","\n","1. **Load Datasets**:\n","   - Reads data from several CSV files using `pandas.read_csv`:\n","     - **Player-Play Data**: Includes key features such as `routeRan`, `inMotionAtBallSnap`, and unique identifiers like `nflId`, `playId`, and `gameId`.\n","     - **Play Data**: Contains contextual play information such as `quarter`, `yardsToGo`, and team possession details.\n","     - **Player Data**: Provides player attributes such as `height`, `weight`, `collegeName`, and `position`.\n","     - **Game Data**: Includes game-level metadata.\n","     - **Tracking Data**: Combines tracking files for all weeks and filters the data for \"BEFORE_SNAP\" frames.\n","\n","2. **Filter Valid Tracking Data**:\n","   - Removes rows with null values in the `event` column to ensure clean tracking data.\n","\n","3. **Merge Datasets**:\n","   - Merges the datasets step-by-step using unique identifiers (`nflId`, `playId`, and `gameId`):\n","     - Player data is merged with player-play data.\n","     - Play data is merged with game data.\n","     - Tracking data is integrated into the consolidated dataset.\n","\n","4. **Feature Engineering**:\n","   - **Distance from QB**: Calculates the Euclidean distance between each player and the quarterback using their x and y coordinates.\n","   - **Distance from Line of Scrimmage**: Computes the absolute difference between a player's x-coordinate and the `absoluteYardlineNumber`.\n","   - **Distance from Sidelines**: Measures the minimum distance between a player and the nearest sideline (field width = 53.3 yards).\n","   - **Seconds Left in Game**: Converts the game clock into total seconds remaining.\n","\n","5. **Prepare Features and Target**:\n","   - Drops unnecessary identifiers (`gameId`, `playId`, `qb_x`, `qb_y`) to avoid data leakage.\n","   - Splits the dataset into features (`x`) and the target variable (`y`), where `y` represents the `routeRan`.\n","\n","6. **Error Handling**:\n","   - Logs errors and raises exceptions if issues occur during data loading or processing.\n","\n","The final output is a tuple containing `x` (features) and `y` (target variable), which are ready for preprocessing and modeling.\n"]},{"cell_type":"code","execution_count":2,"id":"b9f3f25f","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:11:55.938024Z","iopub.status.busy":"2025-01-02T22:11:55.937488Z","iopub.status.idle":"2025-01-02T22:11:55.951313Z","shell.execute_reply":"2025-01-02T22:11:55.949656Z"},"papermill":{"duration":0.022694,"end_time":"2025-01-02T22:11:55.953563","exception":false,"start_time":"2025-01-02T22:11:55.930869","status":"completed"},"tags":[]},"outputs":[],"source":["def load_and_prepare_data():\n","    \"\"\"\n","    Loads, prepares, and merges data from various CSV files.\n","\n","    Returns:\n","        tuple: X (features), y (target), dataframes of the training and testing sets.\n","    \"\"\"\n","    \n","    try:\n","        logging.info(\"Loading datasets...\")\n","\n","        player_play_data = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2025/player_play.csv\")[[\n","            \"routeRan\", \"nflId\", \"playId\", \"gameId\", \"inMotionAtBallSnap\"\n","        ]].dropna(subset=[\"routeRan\"])\n","\n","        play_data = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2025/plays.csv\")[[\n","            \"quarter\", \"down\", \"yardsToGo\", \"possessionTeam\", \"gameClock\", \"preSnapHomeScore\",\n","            \"preSnapVisitorScore\", \"absoluteYardlineNumber\", \"preSnapHomeTeamWinProbability\", \"preSnapVisitorTeamWinProbability\",\n","            \"expectedPoints\", \"offenseFormation\", \"receiverAlignment\", \"gameId\", \"playId\"\n","        ]]\n","\n","        player_data = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2025/players.csv\")[[\n","            \"height\", \"weight\", \"collegeName\", \"nflId\", \"position\"\n","        ]]\n","\n","        game_data = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2025/games.csv\").drop(\n","            [\"season\", \"homeFinalScore\", \"visitorFinalScore\"], axis=1\n","        )\n","\n","        tracking_files = [(f\"/kaggle/input/nfl-big-data-bowl-2025/tracking_week_{i}.csv\") for i in range(1, 10)]\n","        tracking_data_combined = pd.concat(\n","            [\n","                pd.read_csv(file)[\n","                    [\"gameId\", \"playId\", \"nflId\", \"playDirection\", \"x\", \"y\", \"frameType\", \"event\"]\n","                ].query(\"frameType == 'BEFORE_SNAP'\")\n","                for file in tracking_files\n","            ],\n","            ignore_index=True\n","        )\n","\n","        tracking_data_combined = tracking_data_combined[tracking_data_combined[\"event\"].notnull()]\n","\n","        logging.info(\"Merging datasets...\")\n","        player_play_merged = pd.merge(player_play_data, player_data, on=\"nflId\")\n","        play_game_merged = pd.merge(play_data, game_data, on=\"gameId\")\n","\n","        final_data = pd.merge(player_play_merged, play_game_merged, on=[\"playId\", \"gameId\"])\n","        final_data = pd.merge(final_data, tracking_data_combined, on=[\"gameId\", \"playId\", \"nflId\"])\n","\n","        # Calculate distance from QB\n","        qb_positions = final_data[final_data[\"position\"] == \"QB\"][[\"x\", \"y\", \"playId\", \"gameId\"]].rename(\n","            columns={\"x\": \"qb_x\", \"y\": \"qb_y\"}\n","        )\n","        final_data = pd.merge(final_data, qb_positions, on=[\"playId\", \"gameId\"], how=\"left\")\n","        final_data[\"distance_from_qb\"] = np.sqrt(\n","            (final_data[\"x\"] - final_data[\"qb_x\"])**2 + (final_data[\"y\"] - final_data[\"qb_y\"])**2\n","        )\n","\n","        # Calculate distance from line of scrimmage\n","        final_data[\"distance_from_los\"] = np.abs(final_data[\"x\"] - final_data[\"absoluteYardlineNumber\"])\n","\n","        # Calculate distance from sidelines (field width = 53.3 yards)\n","        final_data[\"distance_from_sideline\"] = np.minimum(final_data[\"y\"], 53.3 - final_data[\"y\"])\n","\n","        # Calculate seconds left in the game\n","        def game_clock_to_seconds(clock):\n","            minutes, seconds = map(int, clock.split(\":\"))\n","            return minutes * 60 + seconds\n","\n","        final_data[\"seconds_left_in_game\"] = final_data[\"gameClock\"].apply(game_clock_to_seconds)\n","\n","        #Drop unnecessary columns\n","        final_data = final_data.drop(columns=[\"gameId\", \"playId\", \"qb_x\", \"qb_y\", \"gameClock\", \"frameType\"])\n","\n","        # Create features and target\n","        x = final_data.drop(columns=[\"routeRan\"])\n","        y = final_data[\"routeRan\"]\n","\n","        return x, y\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        raise"]},{"cell_type":"markdown","id":"a25a61b8","metadata":{"papermill":{"duration":0.005226,"end_time":"2025-01-02T22:11:55.964511","exception":false,"start_time":"2025-01-02T22:11:55.959285","status":"completed"},"tags":[]},"source":["## **Step 3: Splitting and Saving Data**\n","\n","This function divides the dataset into training and testing sets and saves them as CSV files for later use. The key steps include:\n","\n","1. **Split Data**:\n","   - Uses `train_test_split` from `sklearn` to split the features (`x`) and target (`y`) into:\n","     - **Training Set**: Used for training the model.\n","     - **Testing Set**: Used for evaluating the model's performance on unseen data.\n","   - Ensures the split maintains the distribution of the target classes by setting `stratify=y`.\n","   - The `test_size` parameter specifies the proportion of the data reserved for testing (default: 20%).\n","\n","2. **Save Data**:\n","   - Writes the resulting training and testing datasets (`x_train`, `x_test`, `y_train`, `y_test`) to CSV files in the `/kaggle/working` directory for later access during training and evaluation.\n","\n","3. **Logging and Error Handling**:\n","   - Logs messages at each step for transparency and troubleshooting.\n","   - Catches and logs any exceptions encountered during the splitting or saving process.\n","\n","The function returns the training and testing datasets as `x_train`, `x_test`, `y_train`, and `y_test`, ensuring the data is ready for preprocessing and modeling.\n"]},{"cell_type":"code","execution_count":3,"id":"cdcbc38b","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:11:55.977016Z","iopub.status.busy":"2025-01-02T22:11:55.976572Z","iopub.status.idle":"2025-01-02T22:11:55.983828Z","shell.execute_reply":"2025-01-02T22:11:55.982432Z"},"papermill":{"duration":0.015935,"end_time":"2025-01-02T22:11:55.985928","exception":false,"start_time":"2025-01-02T22:11:55.969993","status":"completed"},"tags":[]},"outputs":[],"source":["def split_and_save_data(x, y, test_size=0.2, random_state=42):\n","     \"\"\"Splits the data into training and test sets and saves them to CSV files.\"\"\"\n","     try:\n","        logging.info(\"Splitting data into training and test sets...\")\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,\n","                                                            random_state=random_state,\n","                                                            stratify=y)\n","\n","        logging.info(\"Saving training and testing datasets...\")\n","        x_train.to_csv(os.path.join(\"/kaggle/working/x_train.csv\"), index=False)\n","        x_test.to_csv(os.path.join(\"/kaggle/working/x_test.csv\"), index=False)\n","        y_train.to_csv(os.path.join(\"/kaggle/working/y_train.csv\"), index=False)\n","        y_test.to_csv(os.path.join(\"/kaggle/working/y_test.csv\"), index=False)\n","     except Exception as e:\n","            logging.error(f\"An error occurred: {e}\")\n","            raise\n","\n","     return x_train, x_test, y_train, y_test"]},{"cell_type":"markdown","id":"1f4a9dd2","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.005038,"end_time":"2025-01-02T22:11:55.9967","exception":false,"start_time":"2025-01-02T22:11:55.991662","status":"completed"},"tags":[]},"source":["## **Step 4: Main Execution Function**\n","\n","The `main` function orchestrates the key steps of the data preparation process, ensuring a streamlined and organized workflow. The key steps include:\n","\n","1. **Load and Prepare Data**:\n","   - Calls the `load_and_prepare_data` function to read, filter, and merge datasets, resulting in `x` (features) and `y` (target).\n","\n","2. **Split and Save Data**:\n","   - Invokes the `split_and_save_data` function to divide the dataset into training and testing sets.\n","   - Saves the resulting subsets (`x_train`, `x_test`, `y_train`, `y_test`) to CSV files for later use.\n","\n","3. **Logging and Error Handling**:\n","   - Logs progress to ensure visibility into the execution process.\n","   - Captures and logs any exceptions, providing detailed error messages for debugging.\n","\n","4. **Execution Context**:\n","   - Ensures the function runs only when executed as a script (using the `if __name__ == \"__main__\":` block).\n","\n","This function acts as the entry point for the data preparation process, ensuring the data is properly processed and saved for subsequent steps in the workflow.\n"]},{"cell_type":"code","execution_count":4,"id":"e1a65df0","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:11:56.010704Z","iopub.status.busy":"2025-01-02T22:11:56.010343Z","iopub.status.idle":"2025-01-02T22:15:33.602044Z","shell.execute_reply":"2025-01-02T22:15:33.600506Z"},"papermill":{"duration":217.605614,"end_time":"2025-01-02T22:15:33.608488","exception":false,"start_time":"2025-01-02T22:11:56.002874","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["An error occurred: name 'np' is not defined\n"]}],"source":["def main():\n","    \"\"\"Main execution function.\"\"\"\n","    try:\n","        x, y = load_and_prepare_data()\n","        x_train, x_test, y_train, y_test = split_and_save_data(x, y)\n","    \n","        logging.info(\"Program execution completed successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Program terminated with an exception: {e}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","id":"6d9d8223","metadata":{"papermill":{"duration":0.005437,"end_time":"2025-01-02T22:15:33.62024","exception":false,"start_time":"2025-01-02T22:15:33.614803","status":"completed"},"tags":[]},"source":["## **Step 5: Importing Libraries and Configuring the Environment**\n","\n","This step sets up the required libraries and configurations for the project:\n","\n","1. **Importing Libraries**:\n","   - **Pandas**: For data manipulation and analysis.\n","   - **Scikit-learn**:\n","     - `RandomForestClassifier` for building the classification model.\n","     - Metrics such as `accuracy_score`, `classification_report`, `roc_auc_score`, and `f1_score` for evaluating model performance.\n","     - Preprocessing tools like `OneHotEncoder` and `StandardScaler` for feature transformation.\n","     - `Pipeline` and `ColumnTransformer` for building an end-to-end processing and training workflow.\n","     - `SimpleImputer` for handling missing data.\n","     - `train_test_split` for splitting the dataset into training and testing subsets.\n","   - **Joblib**: For saving and loading the trained model pipeline.\n","   - **Matplotlib and Seaborn**: For data visualization and graphical analysis.\n","\n","2. **Configure Logging**:\n","   - Logging is configured to provide real-time feedback on the script's execution, including info-level messages for successful operations and error messages for debugging.\n","\n","This foundational setup ensures that all tools and configurations are in place for the subsequent steps in the machine learning workflow.\n"]},{"cell_type":"code","execution_count":5,"id":"59425272","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:33.632412Z","iopub.status.busy":"2025-01-02T22:15:33.632047Z","iopub.status.idle":"2025-01-02T22:15:34.537762Z","shell.execute_reply":"2025-01-02T22:15:34.536689Z"},"papermill":{"duration":0.913908,"end_time":"2025-01-02T22:15:34.539624","exception":false,"start_time":"2025-01-02T22:15:33.625716","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, confusion_matrix\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","import joblib\n","import logging\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"]},{"cell_type":"markdown","id":"21c3acf1","metadata":{"papermill":{"duration":0.005281,"end_time":"2025-01-02T22:15:34.550822","exception":false,"start_time":"2025-01-02T22:15:34.545541","status":"completed"},"tags":[]},"source":["## **Step 6: Loading and Preprocessing Data**\n","\n","This function performs the following tasks:\n","\n","1. **Data Loading**:\n","   - Reads the training and testing datasets from pre-saved CSV files in the `/kaggle/working/` directory.\n","   - Outputs `x_train`, `x_test`, `y_train`, and `y_test`.\n","\n","2. **Categorical Feature Encoding**:\n","   - Identifies categorical features in the dataset.\n","   - Converts these features into numerical format using `LabelEncoder`, ensuring compatibility with machine learning models.\n","\n","3. **Numerical Feature Normalization**:\n","   - Scales numerical features using `StandardScaler` to standardize the range of numerical columns, improving model performance and convergence.\n","\n","The returned processed datasets are ready for training and evaluation.\n"]},{"cell_type":"code","execution_count":6,"id":"02352c83","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.5636Z","iopub.status.busy":"2025-01-02T22:15:34.563107Z","iopub.status.idle":"2025-01-02T22:15:34.568695Z","shell.execute_reply":"2025-01-02T22:15:34.567649Z"},"papermill":{"duration":0.013644,"end_time":"2025-01-02T22:15:34.570415","exception":false,"start_time":"2025-01-02T22:15:34.556771","status":"completed"},"tags":[]},"outputs":[],"source":["def load_and_preprocess_data():\n","    \"\"\"Loads, preprocesses, and returns the training and testing datasets.\"\"\"\n","    try:\n","        logging.info(\"Loading datasets...\")\n","        x_train = pd.read_csv(\"/kaggle/working/x_train.csv\")\n","        x_test = pd.read_csv(\"/kaggle/working/x_test.csv\")\n","        y_train = pd.read_csv(\"/kaggle/working/y_train.csv\").squeeze()\n","        y_test = pd.read_csv(\"/kaggle/working/y_test.csv\").squeeze()\n","\n","        logging.info(\"Data loaded successfully.\")\n","        return x_train, x_test, y_train, y_test\n","    \n","    except Exception as e:\n","        logging.error(f\"Error loading data: {e}\")\n","        raise"]},{"cell_type":"markdown","id":"870e8176","metadata":{"papermill":{"duration":0.005188,"end_time":"2025-01-02T22:15:34.581062","exception":false,"start_time":"2025-01-02T22:15:34.575874","status":"completed"},"tags":[]},"source":["## **Step 7: Creating the Preprocessing and Training Pipeline**\n","\n","This function defines and returns a complete preprocessing and training pipeline. The pipeline is composed of the following steps:\n","\n","1. **Identify Column Types**:\n","   - Separates columns into **numerical** and **categorical** categories.\n","\n","2. **Define Preprocessing for Numerical Features**:\n","   - **Imputation**: Fills missing values with the mean of the column using `SimpleImputer`.\n","   - **Scaling**: Standardizes numerical features using `StandardScaler`.\n","\n","3. **Define Preprocessing for Categorical Features**:\n","   - **Imputation**: Fills missing values with the most frequent category using `SimpleImputer`.\n","   - **One-Hot Encoding**: Encodes categorical features into binary (one-hot) vectors using `OneHotEncoder`.\n","\n","4. **Combine Preprocessing Steps**:\n","   - Uses `ColumnTransformer` to apply the appropriate transformations to numerical and categorical columns.\n","\n","5. **Build the Full Pipeline**:\n","   - Adds a `RandomForestClassifier` as the classifier to the pipeline with pre-defined hyperparameters (e.g., `n_estimators=50`, `max_depth=10`).\n","   - Ensures the entire workflow, from preprocessing to training, can be executed in a single pipeline.\n","\n","The output pipeline can be directly used to preprocess data and train the classifier in one step.\n"]},{"cell_type":"code","execution_count":7,"id":"22bad337","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.59336Z","iopub.status.busy":"2025-01-02T22:15:34.593001Z","iopub.status.idle":"2025-01-02T22:15:34.599936Z","shell.execute_reply":"2025-01-02T22:15:34.599006Z"},"papermill":{"duration":0.015047,"end_time":"2025-01-02T22:15:34.601542","exception":false,"start_time":"2025-01-02T22:15:34.586495","status":"completed"},"tags":[]},"outputs":[],"source":["def create_pipeline(x_train):\n","    \"\"\"Creates a preprocessing pipeline and returns it.\"\"\"\n","    try:\n","        logging.info(\"Creating preprocessing pipeline...\")\n","        numeric_columns = x_train.select_dtypes(include=['float64', 'int64']).columns\n","        categorical_columns = x_train.select_dtypes(include=['object', 'bool']).columns\n","\n","        # Define preprocessing steps\n","        numeric_transformer = Pipeline(steps=[\n","            ('imputer', SimpleImputer(strategy='mean')),\n","            ('scaler', StandardScaler())\n","        ])\n","\n","        categorical_transformer = Pipeline(steps=[\n","            ('imputer', SimpleImputer(strategy='most_frequent')),\n","            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","        ])\n","\n","        # Column transformer\n","        preprocessor = ColumnTransformer(\n","            transformers=[\n","                ('num', numeric_transformer, numeric_columns),\n","                ('cat', categorical_transformer, categorical_columns)\n","            ]\n","        )\n","\n","        # Pipeline\n","        pipeline = Pipeline(steps=[\n","            ('preprocessor', preprocessor),\n","            ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100, max_depth=20))\n","        ])\n","\n","        return pipeline\n","        \n","    except Exception as e:\n","        logging.error(f\"Error creating pipeline: {e}\")\n","        raise"]},{"cell_type":"markdown","id":"b833c23e","metadata":{"papermill":{"duration":0.005237,"end_time":"2025-01-02T22:15:34.612364","exception":false,"start_time":"2025-01-02T22:15:34.607127","status":"completed"},"tags":[]},"source":["## **Visualizing the Effect of `max_depth` on Model Accuracy**\n","\n","This function demonstrates the impact of varying the `max_depth` parameter on the accuracy of the RandomForestClassifier. By testing different tree depths, we can identify the optimal depth that balances both train and test accuracy.\n","\n","#### Key Steps:\n","1. **Define Depth Range**:\n","   - The function evaluates tree depths `[5, 10, 15, 20]`.\n","\n","2. **Pipeline Setup**:\n","   - A pipeline is created for each depth value using `create_pipeline()`.\n","   - The `max_depth` parameter of the classifier is adjusted dynamically.\n","\n","3. **Train and Evaluate**:\n","   - The model is trained on the training set for each depth value.\n","   - Accuracy is computed for both the training and testing datasets.\n","\n","4. **Visualization**:\n","   - Results are plotted with tree depth on the x-axis and accuracy on the y-axis.\n","   - Separate lines are plotted for **Train Accuracy** and **Test Accuracy**.\n","\n","#### Output:\n","The chart provides insights into the trade-off between underfitting (low depth) and overfitting (high depth). The ideal `max_depth` corresponds to the point where the test accuracy plateaus or reaches its peak, without excessive overfitting.\n","\n","Use this visualization to select the optimal `max_depth` value for tuning your RandomForest model.\n"]},{"cell_type":"code","execution_count":8,"id":"4f575d09","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.625319Z","iopub.status.busy":"2025-01-02T22:15:34.624868Z","iopub.status.idle":"2025-01-02T22:15:34.632617Z","shell.execute_reply":"2025-01-02T22:15:34.631547Z"},"papermill":{"duration":0.016681,"end_time":"2025-01-02T22:15:34.634522","exception":false,"start_time":"2025-01-02T22:15:34.617841","status":"completed"},"tags":[]},"outputs":[],"source":["def visualize_optimal_depth(x_train, y_train, x_test, y_test):\n","    \"\"\"\n","    Visualizes the effect of max_depth on model accuracy to find the optimal depth.\n","    Args:\n","        x_train: Training features.\n","        y_train: Training labels.\n","        x_test: Testing features.\n","        y_test: Testing labels.\n","    \"\"\"\n","    try:\n","        print(\"Visualizing the effect of max_depth on model accuracy...\")\n","\n","        depths = [3, 5, 10, 15, 20]  # Depth values to test\n","        train_scores = []\n","        test_scores = []\n","\n","        for depth in depths:\n","            # Create a pipeline with the current depth\n","            pipeline = create_pipeline(x_train)\n","            pipeline.named_steps['classifier'].set_params(max_depth=depth)\n","            \n","            # Train the model\n","            pipeline.fit(x_train, y_train)\n","            \n","            # Evaluate on train and test sets\n","            train_scores.append(pipeline.score(x_train, y_train))\n","            test_scores.append(pipeline.score(x_test, y_test))\n","\n","        # Plot the results\n","        plt.figure(figsize=(10, 6))\n","        plt.plot([d if d is not None else 25 for d in depths], train_scores, label='Train Accuracy', marker='o')\n","        plt.plot([d if d is not None else 25 for d in depths], test_scores, label='Test Accuracy', marker='o')\n","        plt.title(\"Effect of max_depth on Model Accuracy\")\n","        plt.xlabel(\"Tree Depth\")\n","        plt.ylabel(\"Accuracy\")\n","        plt.legend()\n","        plt.grid()\n","        plt.tight_layout()\n","        plt.show()\n","\n","    except Exception as e:\n","        logging.error(f\"Error visualizing optimal depth: {e}\")\n","        raise\n"]},{"cell_type":"markdown","id":"d5463d07","metadata":{"papermill":{"duration":0.005167,"end_time":"2025-01-02T22:15:34.645549","exception":false,"start_time":"2025-01-02T22:15:34.640382","status":"completed"},"tags":[]},"source":["## **Step 8: Training and Evaluating the Model**\n","\n","This function trains the `RandomForestClassifier` using the provided pipeline and evaluates its performance on the test dataset. The key steps are:\n","\n","1. **Training the Model**:\n","   - The pipeline, which includes both preprocessing and the classifier, is trained on the `x_train` and `y_train` datasets.\n","\n","2. **Model Evaluation**:\n","   - The trained model makes predictions (`y_pred`) on the test dataset (`x_test`).\n","   - Calculates performance metrics:\n","     - **Accuracy**: The proportion of correct predictions over the total predictions.\n","     - **F1 Score**: The weighted average of precision and recall, accounting for imbalanced classes.\n","\n","3. **Generate Classification Report**:\n","   - Displays precision, recall, F1-score, and support for each class, providing a detailed breakdown of the model's performance.\n","\n","4. **Logging and Output**:\n","   - Logs the accuracy and F1 score for tracking.\n","   - Returns the trained pipeline for further use.\n","\n","This function ensures the model's performance is quantified and provides insights into its effectiveness in predicting the target variable.\n"]},{"cell_type":"code","execution_count":9,"id":"e0b6e8f6","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.658103Z","iopub.status.busy":"2025-01-02T22:15:34.657703Z","iopub.status.idle":"2025-01-02T22:15:34.666511Z","shell.execute_reply":"2025-01-02T22:15:34.665325Z"},"papermill":{"duration":0.01725,"end_time":"2025-01-02T22:15:34.66823","exception":false,"start_time":"2025-01-02T22:15:34.65098","status":"completed"},"tags":[]},"outputs":[],"source":["def train_and_evaluate_model(pipeline, x_train, x_test, y_train, y_test, importance_threshold=0.01):\n","    \"\"\"\n","    Trains and evaluates a RandomForestClassifier model, dropping low-importance features.\n","    Args:\n","        pipeline: The preprocessing and training pipeline.\n","        x_train: Training features.\n","        x_test: Testing features.\n","        y_train: Training labels.\n","        y_test: Testing labels.\n","        importance_threshold: Minimum importance value for a feature to be retained.\n","    \"\"\"\n","    try:\n","        logging.info(\"Training RandomForestClassifier model...\")\n","        pipeline.fit(x_train, y_train)\n","\n","        # Extract feature importances\n","        rf_model = pipeline.named_steps['classifier']\n","        feature_importances = rf_model.feature_importances_\n","        feature_names = (\n","            pipeline.named_steps['preprocessor'].transformers_[0][2].tolist() +\n","            pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out().tolist()\n","        )\n","        importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n","        importance_df = importance_df.sort_values('importance', ascending=False)\n","\n","        # Drop low-importance features\n","        low_importance_features = importance_df[importance_df['importance'] < importance_threshold]['feature'].tolist()\n","        logging.info(f\"Dropping {len(low_importance_features)} low-importance features: {low_importance_features}\")\n","\n","        # Update datasets\n","        x_train = x_train.drop(columns=low_importance_features, errors='ignore')\n","        x_test = x_test.drop(columns=low_importance_features, errors='ignore')\n","\n","        # Recreate and train pipeline\n","        pipeline = create_pipeline(x_train)\n","        pipeline.fit(x_train, y_train)\n","\n","        logging.info(\"Evaluating model on the testing set...\")\n","        y_pred = pipeline.predict(x_test)\n","\n","        # Evaluate performance\n","        test_accuracy = accuracy_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        logging.info(f\"Testing Set Accuracy: {test_accuracy}\")\n","        logging.info(f\"F1 Score: {f1}\")\n","\n","        print(\"Testing Set Accuracy:\", test_accuracy)\n","        print(\"F1 Score:\", f1)\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(y_test, y_pred))\n","\n","        return pipeline\n","\n","    except Exception as e:\n","        logging.error(f\"Error training or evaluating model: {e}\")\n","        raise\n"]},{"cell_type":"markdown","id":"fc1b1535","metadata":{"papermill":{"duration":0.005393,"end_time":"2025-01-02T22:15:34.679478","exception":false,"start_time":"2025-01-02T22:15:34.674085","status":"completed"},"tags":[]},"source":["## **Step 9: Visualizing Model Results and Performance**\n","\n","This function generates detailed visualizations to evaluate the model's predictions, understand feature importance, and uncover key insights relevant to NFL data analysis. The key steps include:\n","\n","### 1. **Confusion Matrix**:\n","   - A heatmap displays the confusion matrix, highlighting the comparison between **true labels** and **predicted labels**.\n","   - This helps identify which routes are commonly misclassified by the model.\n","\n","### 2. **Classification Report Heatmap**:\n","   - A heatmap visualization of the classification report, including **precision**, **recall**, and **f1-score** for each route.\n","   - Offers a detailed breakdown of the model's performance on each class (route).\n","\n","### 3. **Feature Importance**:\n","   - Extracts the `RandomForestClassifier` from the pipeline using `pipeline.named_steps['classifier']`.\n","   - Computes the importance of each feature in the model.\n","   - Combines feature names from the preprocessing pipeline (numerical and one-hot encoded categorical features) with their corresponding importance values.\n","   - Plots the top `n` features (default: 10) in descending order of importance to identify the most influential factors for predicting receiver routes.\n","\n","### 4. **Distribution of Predicted Routes**:\n","   - Displays the frequency of predicted routes using a count plot.\n","   - Highlights the balance (or imbalance) in the predicted route classes, providing insights into model predictions.\n","\n","### 5. **Log and Handle Errors**:\n","   - Logs the visualization process for better transparency and debugging.\n","   - Catches and logs any exceptions that may occur during the visualization process.\n","\n","### **Insights**:\n","- These visualizations allow us to:\n","  - **Evaluate Model Performance**: Understand how well the model predicts each route and where it might struggle.\n","  - **Feature Insights**: Identify key features that drive the model's predictions, which can inform future feature engineering.\n","  - **Class Distribution**: Check for any biases in predictions across different routes.\n","  - **Guide Improvements**: Use the results to refine the model and address any weaknesses.\n","\n","This step is essential for interpreting the model's results and ensuring its predictions align with the goals of the NFL Big Data Bowl project.\n"]},{"cell_type":"code","execution_count":10,"id":"9c73926a","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.691923Z","iopub.status.busy":"2025-01-02T22:15:34.691551Z","iopub.status.idle":"2025-01-02T22:15:34.701612Z","shell.execute_reply":"2025-01-02T22:15:34.700591Z"},"papermill":{"duration":0.018429,"end_time":"2025-01-02T22:15:34.703399","exception":false,"start_time":"2025-01-02T22:15:34.68497","status":"completed"},"tags":[]},"outputs":[],"source":["def visualize_model_results(x_train, y_test, y_pred, pipeline, top_n_features=10):\n","    \"\"\"\n","    Generates visualizations of model performance, feature importance, and NFL-relevant data insights.\n","    Args:\n","        x_train: Training features used in the model.\n","        y_test: True labels for the test set.\n","        y_pred: Predicted labels from the model.\n","        pipeline: Trained machine learning pipeline.\n","        top_n_features: Number of top features to display in feature importance plots.\n","    \"\"\"\n","    try:\n","        logging.info(\"Generating visualizations...\")\n","\n","        # Confusion Matrix\n","        plt.figure(figsize=(10, 7))\n","        cm = confusion_matrix(y_test, y_pred)\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=y_test.unique(), yticklabels=y_test.unique())\n","        plt.title(\"Confusion Matrix\")\n","        plt.xlabel(\"Predicted Labels\")\n","        plt.ylabel(\"True Labels\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Classification Report Heatmap\n","        report = classification_report(y_test, y_pred, output_dict=True)\n","        report_df = pd.DataFrame(report).transpose()\n","        plt.figure(figsize=(10, 6))\n","        sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n","        plt.title(\"Classification Report Heatmap\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Feature Importance\n","        rf_model = pipeline.named_steps['classifier']\n","        feature_importances = rf_model.feature_importances_\n","        feature_names = (\n","            pipeline.named_steps['preprocessor'].transformers_[0][2].tolist() +\n","            pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out().tolist()\n","        )\n","        importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n","        importance_df = importance_df.sort_values('importance', ascending=False)\n","\n","        plt.figure(figsize=(12, 8))\n","        sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(top_n_features), palette=\"viridis\")\n","        plt.title(\"Top Feature Importance\")\n","        plt.xlabel(\"Feature Importance\")\n","        plt.ylabel(\"Feature Names\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Distribution of Predicted Routes\n","        plt.figure(figsize=(10, 6))\n","        sns.countplot(x=y_pred, palette=\"muted\")\n","        plt.title(\"Distribution of Predicted Routes\")\n","        plt.xlabel(\"Predicted Route\")\n","        plt.ylabel(\"Frequency\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    except Exception as e:\n","        logging.error(f\"Error during visualization: {e}\")\n","        raise\n"]},{"cell_type":"markdown","id":"bd90f099","metadata":{"papermill":{"duration":0.005665,"end_time":"2025-01-02T22:15:34.715016","exception":false,"start_time":"2025-01-02T22:15:34.709351","status":"completed"},"tags":[]},"source":["## **Step 10: Main Execution Function**\n","\n","The `main` function orchestrates the complete workflow of the machine learning pipeline, ensuring each component integrates seamlessly. The key steps include:\n","\n","### 1. **Load and Preprocess Data**:\n","   - Calls `load_and_preprocess_data` to load the training and testing datasets.\n","   - Handles preprocessing, including encoding categorical variables and scaling numerical features.\n","\n","### 2. **Create the Pipeline**:\n","   - Utilizes `create_pipeline` to define the machine learning pipeline, combining feature preprocessing steps with a `RandomForestClassifier`.\n","\n","### 3. **Train and Evaluate the Model**:\n","   - Invokes `train_and_evaluate_model` to train the pipeline on the training data and evaluate its performance on the test data.\n","   - Measures key performance metrics such as accuracy, precision, recall, and F1-score.\n","\n","### 4. **Visualize Optimal Depth**:\n","   - Runs `visualize_optimal_depth` to analyze the impact of the `max_depth` parameter on model accuracy.\n","   - Identifies the depth value that balances model complexity and performance.\n","\n","### 5. **Visualize Model Results**:\n","   - Executes `visualize_model_results` to generate critical visualizations, including:\n","     - **Confusion Matrix**: Highlights misclassifications.\n","     - **Classification Report Heatmap**: Summarizes precision, recall, and F1-scores.\n","     - **Feature Importance Plot**: Identifies the top features driving predictions.\n","     - **Predicted Route Distribution**: Shows the balance of route predictions.\n","\n","### 6. **Error Handling and Logging**:\n","   - Logs the progress of each step and captures any exceptions encountered during execution, ensuring traceability and debugging support.\n","\n","### **Insights**:\n","This function integrates all prior steps into a cohesive workflow. By training, evaluating, and visualizing the model's performance and key parameters, the `main` function ensures the project's goals are met with clarity and accuracy. This centralized structure provides an effective foundation for iterative improvements and further exploration.\n"]},{"cell_type":"code","execution_count":11,"id":"a5997ca9","metadata":{"execution":{"iopub.execute_input":"2025-01-02T22:15:34.728221Z","iopub.status.busy":"2025-01-02T22:15:34.727807Z","iopub.status.idle":"2025-01-02T22:15:34.734692Z","shell.execute_reply":"2025-01-02T22:15:34.733617Z"},"papermill":{"duration":0.015314,"end_time":"2025-01-02T22:15:34.736314","exception":false,"start_time":"2025-01-02T22:15:34.721","status":"completed"},"tags":[]},"outputs":[],"source":["def main():\n","    \"\"\"Main execution function.\"\"\"\n","    try:\n","        x_train, x_test, y_train, y_test = load_and_preprocess_data()\n","        pipeline = create_pipeline(x_train)\n","\n","        # Train and evaluate model\n","        pipeline = train_and_evaluate_model(pipeline, x_train, x_test, y_train, y_test)\n","\n","        # Visualize model results\n","        visualize_model_results(\n","        x_train=x_train,                 # Training features\n","        y_test=y_test,                   # True labels for the test set\n","        y_pred=pipeline.predict(x_test), # Predicted labels from the pipeline\n","        pipeline=pipeline,               # The trained pipeline object\n","        top_n_features=10                # Number of top features to display in the feature importance plot\n",")\n","\n","        # Visualize optimal depth\n","        visualize_optimal_depth(x_train, x_test, y_train, y_test)\n","\n","        logging.info(\"Program execution completed successfully.\")\n","\n","    except Exception as e:\n","        logging.error(f\"Program terminated with an exception: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","id":"ee49fda9","metadata":{"papermill":{"duration":0.005375,"end_time":"2025-01-02T22:15:34.747799","exception":false,"start_time":"2025-01-02T22:15:34.742424","status":"completed"},"tags":[]},"source":["## **Updated Final Summary and Insights**\n","\n","This project implemented a machine learning pipeline, utilizing a Random Forest Classifier, to predict NFL receiver routes using data from the NFL Big Data Bowl 2025. The integration of multiple datasets on players, games, and tracking data provided comprehensive insights into route prediction. Below are the key takeaways and updates based on the latest results.\n","\n","---\n","\n","### **Performance Metrics**\n","- **Testing Set Accuracy**: 57.54%\n","- **F1 Score**: 57.55%\n","- **Detailed Results**:\n","  - **Top-performing routes**:\n","    - **WHEEL**: Precision (92%), Recall (65%), F1 (76%) – highest-performing route despite limited data.\n","    - **FLAT**: Precision (80%), Recall (61%), F1 (69%) – solid overall performance.\n","  - **Moderate-performing routes**:\n","    - **CROSS**: Precision (55%), Recall (67%), F1 (60%).\n","    - **GO**: Balanced performance with F1 (60%).\n","  - **Low-performing routes**:\n","    - **ANGLE**: Precision (46%), Recall (89%), F1 (60%) – over-prediction remains a challenge.\n","    - **OUT**: Precision (65%), Recall (39%), F1 (48%) – under-prediction continues to limit accuracy.\n","\n","---\n","\n","### **Key Visualizations**\n","1. **Confusion Matrix**:\n","   - Revealed consistent challenges in distinguishing between similar routes, such as **OUT** vs. **CROSS**.\n","2. **Feature Importance**:\n","   - Key influential features:\n","     - **Distance from Line of Scrimmage (LOS)**\n","     - **Seconds Left in Game**\n","     - **Receiver Alignment**\n","   - Dropping low-importance features helped mitigate overfitting and marginally improved accuracy.\n","3. **Classification Report Heatmap**:\n","   - Highlighted varying levels of precision and recall across different routes.\n","4. **Predicted Route Distribution**:\n","   - Showed a reasonably balanced prediction distribution, though some routes remain underrepresented.\n","\n","---\n","\n","### **Key Challenges**\n","- **Class Imbalance**:\n","  - Underrepresented routes (e.g., **WHEEL**, **SCREEN**) remained difficult to predict with consistent accuracy.\n","- **Feature Overlap**:\n","  - Routes with similar feature profiles, such as **POST** and **GO**, continued to confuse the model.\n","- **Performance Plateau**:\n","  - Accuracy and F1 score improvements are incremental, suggesting diminishing returns with the current model setup.\n","\n","---\n","\n","### **Improvements and Next Steps**\n","1. **Enhanced Feature Engineering**:\n","   - Include advanced features such as player acceleration, proximity to defenders, and motion patterns.\n","2. **Alternative Models**:\n","   - Test gradient boosting methods (e.g., XGBoost, LightGBM) for improved decision boundaries.\n","3. **Data Augmentation**:\n","   - Generate synthetic samples for underrepresented classes like **WHEEL** and **SCREEN**.\n","4. **Domain Knowledge Integration**:\n","   - Leverage football-specific features such as route clusters, defensive coverage type, and quarterback tendencies.\n","5. **Hyperparameter Optimization**:\n","   - Conduct a more granular grid search to fine-tune key parameters like max depth and estimators.\n","\n","---\n","\n","### **Relevance to NFL Teams**\n","This model demonstrates practical utility for route prediction, offering insights into:\n","- **Game Strategy**: Identifying tendencies and counter-strategies for opponent receiver routes.\n","- **Player Performance Evaluation**: Assessing route execution and tendencies under different conditions.\n","- **In-Game Decision Support**: Enhancing real-time analytics to inform strategic adjustments during games.\n","\n","---\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9816926,"sourceId":84175,"sourceType":"competition"}],"dockerImageVersionId":30822,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":223.811814,"end_time":"2025-01-02T22:15:35.475246","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-02T22:11:51.663432","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}